Neural networks trained the way we train them today can suffer from “adversarial attacks”, where the attacker can craft some very small change to an image (imperceptible to humans) that, when input to the network, arbitrarily changes what the network predicts. For instance, an image of a stop sign could be carefully manipulated such that the network sees a green light instead, which would of course be catastrophic. Several methods have been proposed to address this issue with neural networks, but most rely on empirical results, so it hasn’t been very clear how these methods are solving this issue under the hood. For this project, I derived some math (along with another Ph.D. student) to estimate how likely a certain neural network thinks this image is. For instance, an image of a dog would be very common so our method would say it has a high probability, but if you add some small noise to the image, a standard network would say that image is unlikely (and thus be likely to predict it wrong). What we found from these experiments is that adversarial robustness methods (protecting from adversarial attacks) tend to make the model think that small noise / adversarial attacks have a higher probability (i.e., the network considers it a possibility, so it classifies it correctly). This work furthers the field’s understanding of adversarial robustness techniques and what they accomplish. We submitted and published it in the ECCV 2020 Workshop on Adversarial Robustness in the Real World and received the NVIDIA Best Paper Runner-Up Award for the work.
